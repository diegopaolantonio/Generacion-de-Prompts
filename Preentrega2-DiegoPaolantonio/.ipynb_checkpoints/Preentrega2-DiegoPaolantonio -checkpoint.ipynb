{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0_wDuENO-z1",
    "outputId": "e6966b74-84f6-4e8c-8b16-af812eba4592"
   },
   "outputs": [],
   "source": [
    "# Utilizo gemini para la generacion de texto y para esta entrega utilizo el prompt de imagen generado en \"app.leonardo.ai\" que genera imagenes gratuitas.\n",
    "# Deje la implementacion con openai para la generacion de imagenes para utilizar en la entrega final para minimizar los costos de la generacion.\n",
    "# Inclui en la carpeta 2 de las imagenes que se generaron con el prompt generado para la imagen.\n",
    "\n",
    "\n",
    "#!pip install -U google-generativeai\n",
    "#!pip install openai==0.28\n",
    "\n",
    "import google.generativeai as genai\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0U7Dw_u0PgMo"
   },
   "outputs": [],
   "source": [
    "# https://aistudio.google.com/app/apikey Get an api key\n",
    "genai.configure(api_key=\"\") # your sekret key gemini\n",
    "\n",
    "# https://platform.openai.com/api-keys Get an api key\n",
    "openai.api_key = \"\" # your sekret key openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RX7VxO0YQ4Du"
   },
   "outputs": [],
   "source": [
    "# Make a text from an prompt (gemini)\n",
    "def generate_text_with_gemini(prompt):\n",
    "    # Generate content\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    response = model.generate_content(f\"{prompt} resume la respuesta en 100 palabras maximo\")\n",
    "\n",
    "    # Get the response's text\n",
    "    generated_text = response.candidates[0].content.parts[0].text    \n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "u2xzh8pnQ742",
    "outputId": "7cb8f9b2-51f3-48ef-da2f-14fb42009f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido a IconText el asistente generador de logos y descripciones profesionales con IA.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Que es lo que esta por describir: Proyecto o Empresa? empresa\n",
      "\n",
      "Nombre del Proyecto o Empresa? (si no tiene nombre asignado dejar en blanco) Experto Ing\n",
      "\n",
      "Por favor describa brevemente el proyecto/empresa para la que necesita generar el logo y la descripsion profesional: Empresa de automatizacion industrial, que se dedica principalmente a programacion de PLC y HMI para la industria automotriz.\n",
      "\n",
      "Por favor indique el tono de la generacion, por defecto es profesional (ejemplos familiar, sobrio, profesional, amigable, etc.): \n",
      "\n",
      "La descricion se genera en ingles por defecto, indique los idiomas adicionales que necesita generar el texto: español y portugues\n",
      "\n",
      "Por favor el pais en que se comersializa o brinda servicios: Argentina y Brasil\n",
      "\n",
      "Si tiene alguna preferencia para el logo especifique aqui (Ejemplo moderno, que incluya un auto, utilizar tonos de azul, etc): Un aguila volando, diseño moderno, con tonos rojo y blanco.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Prompt Gemini: Experto Ing is a leading industrial automation company specializing in PLC and HMI programming for the automotive industry in Argentina and Brazil. Our team of experienced engineers delivers tailored solutions for improved efficiency, safety, and productivity. We leverage cutting-edge technologies to optimize processes, minimize downtime, and enhance overall plant performance.  Partner with us to unlock the full potential of your automotive operations. \n",
      "\n",
      "## Español:\n",
      "\n",
      "Experto Ing es una empresa líder en automatización industrial especializada en programación de PLC y HMI para la industria automotriz en Argentina y Brasil. Nuestro equipo de ingenieros experimentados ofrece soluciones personalizadas para mejorar la eficiencia, la seguridad y la productividad. Aprovechamos tecnologías de vanguardia para optimizar procesos, minimizar tiempos de inactividad y mejorar el rendimiento general de la planta.  Asóciate con nosotros para liberar todo el potencial de tus operaciones automotrices.\n",
      "\n",
      "## Portugués:\n",
      "\n",
      "Experto Ing é uma empresa líder em automação industrial especializada em programação de PLC e HMI para a indústria automotiva na Argentina e no Brasil. Nossa equipe de engenheiros experientes oferece soluções personalizadas para melhorar a eficiência, a segurança e a produtividade. Aproveitamos tecnologias de ponta para otimizar processos, minimizar o tempo de inatividade e melhorar o desempenho geral da planta.  Faça parceria conosco para liberar todo o potencial de suas operações automotivas. \n",
      "\n",
      "\n",
      "\n",
      "Prompt para la imagen: Generate a logo for a company named Experto Ing. The logo should feature a modern design with a white eagle soaring against a red background. Experto Ing is a leading industrial automation company specializing in PLC and HMI programming for the automotive industry in Argentina and Brazil. The logo should reflect their expertise, innovation, and commitment to delivering high-quality solutions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bienvenido a IconText el asistente generador de logos y descripciones profesionales con IA.\")\n",
    "\n",
    "\n",
    "# Make a prompt\n",
    "while True:\n",
    "    prompt_objetivo = input(\"\\nQue es lo que esta por describir: Proyecto o Empresa?\").lower()\n",
    "    if prompt_objetivo == \"empresa\":\n",
    "        break\n",
    "    if prompt_objetivo == \"proyecto\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"Respuesta no valida\")\n",
    "# Ej \"Experto ing\"\n",
    "\n",
    "prompt_nombre = input(\"\\nNombre del Proyecto o Empresa? (si no tiene nombre asignado dejar en blanco)\")\n",
    "if prompt_nombre == \"\":\n",
    "    while True:\n",
    "        prompt_gen_nombre = input(\"Desea generar un nombre clave? y/n\").lower()\n",
    "        if prompt_gen_nombre == \"y\":\n",
    "            break\n",
    "        if prompt_gen_nombre == \"n\":\n",
    "            break\n",
    "\n",
    "else:\n",
    "    prompt_gen_nombre = \"\";\n",
    "\n",
    "prompt_descripcion = input(\"\\nPor favor describa brevemente el proyecto/empresa para la que necesita generar el logo y la descripsion profesional:\") #\"Describe lo que necesita el cliente para generar un texto y logo.\"\n",
    "# Ejemplo \"Empresa de automatizacion industrial, que se dedica principalmente a programacion de PLC y HMI para la industria automotriz.\"\n",
    "\n",
    "prompt_tono = input(\"\\nPor favor indique el tono de la generacion, por defecto es profesional (ejemplos familiar, sobrio, profesional, amigable, etc.):\") #\"Esto genera que el tono del texto y logo generado sea mejor a lo que se esta buscando.\"\n",
    "# Si el tono es \"profesional\" dejar en blanco si no escribir el tono deseado.\n",
    "\n",
    "prompt_idioma = input(\"\\nLa descricion se genera en ingles por defecto, indique los idiomas adicionales que necesita generar el texto:\") #\"Esto le da a IA los idiomas a los que debe generar el texto.\"\n",
    "# Ej \"español y portugues\"\n",
    "\n",
    "while True:\n",
    "    prompt_pais = input(\"\\nPor favor el pais en que se comersializa o brinda servicios:\") #\"Esto ayuda a la IA a generar el texto y el logo a la cultura correcta.\"\n",
    "    if prompt_pais != \"\":\n",
    "        break\n",
    "    print(\"Ingrese un pais\")\n",
    "# Ej \"Argentina y Brasil\"\n",
    "\n",
    "prompt_preferencia = input(\"\\nSi tiene alguna preferencia para el logo especifique aqui (Ejemplo moderno, que incluya un auto, utilizar tonos de azul, etc):\")\n",
    "# Ej \"Un aguila volando, diseño moderno, con tonos rojo y blanco.\"\n",
    "\n",
    "\n",
    "# Generate code name\n",
    "if prompt_gen_nombre == \"y\":\n",
    "    prompt_nombre = generate_text_with_gemini(f\"Generar un nombre para un proyecto o empresa que se describe asi: {prompt_descripcion}, devolver un nombre generado y solo eso, sin descripcion adicional.\")\n",
    "    print(f\"Nombre propuesto: {prompt_nombre}\")\n",
    "\n",
    "\n",
    "# Extra\n",
    "if prompt_tono == \"\":\n",
    "    prompt_tono = \"profesional\"\n",
    "\n",
    "\n",
    "# Generate prompt for generative text\n",
    "if prompt_objetivo == \"empresa\":\n",
    "    texto_para_gen_prompt_texto = \"Generar una breve descripcion para una empresa,\"\n",
    "if prompt_objetivo == \"proyecto\":\n",
    "    texto_para_gen_prompt_texto = \"Generar una breve descripcion para un proyecto,\"\n",
    "\n",
    "if prompt_nombre != \"\":\n",
    "    texto_para_gen_prompt_texto = texto_para_gen_prompt_texto + f\" que se llama {prompt_nombre},\"\n",
    "\n",
    "texto_para_gen_prompt_texto = texto_para_gen_prompt_texto + f\" que se describe como {prompt_descripcion}, generar con un tono {prompt_tono} y para {prompt_pais} pais, y en ingles.\"\n",
    "texto_para_gen_prompt_texto = texto_para_gen_prompt_texto + \" Devolver el texto generado solo, sin descripciones ni consultas extras.\"\n",
    "\n",
    "\n",
    "# Use the function\n",
    "response_of_function_gemini = generate_text_with_gemini(texto_para_gen_prompt_texto)\n",
    "\n",
    "if prompt_idioma != \"\":\n",
    "    response_of_function_gemini_idiomas = generate_text_with_gemini(f\"Traducir {response_of_function_gemini} en {prompt_idioma}\")\n",
    "\n",
    "# Output\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Prompt Gemini: {response_of_function_gemini}\")\n",
    "if prompt_idioma != \"\":\n",
    "    print(response_of_function_gemini_idiomas)\n",
    "\n",
    "\n",
    "# Generate prompt for generative image\n",
    "\n",
    "# Make a prompt\n",
    "texto_para_gen_prompt_imagen = f\"Generar un prompt para generar un logo para una {prompt_objetivo}\"\n",
    "\n",
    "if prompt_nombre != \"\":\n",
    "    texto_para_gen_prompt_imagen = texto_para_gen_prompt_imagen + f\" que incluya el nombre {prompt_nombre}\"\n",
    "\n",
    "texto_para_gen_prompt_imagen = texto_para_gen_prompt_imagen + f\", considenrando esta descripcion {response_of_function_gemini}\"\n",
    "\n",
    "if prompt_preferencia != \"\":\n",
    "    texto_para_gen_prompt_imagen = texto_para_gen_prompt_imagen + f\" y estas acotaciones presentadas por el usuario {prompt_preferencia}\"\n",
    "\n",
    "texto_para_gen_prompt_imagen = texto_para_gen_prompt_imagen + \", el prompt tiene que generarse con la informacion presentada aqui, y sin pedir informacion adicional, y el unico texto que debe ser incluido es el nombre y ningun otro texto. Debe entregarse solo el prompt, sin comentarios y en ingles.\"\n",
    "\n",
    "# Use the function\n",
    "response_of_function_gemini_imagen = generate_text_with_gemini(texto_para_gen_prompt_imagen)\n",
    "\n",
    "# Output\n",
    "print(\"\\n\")\n",
    "print(f\"Prompt para la imagen: {response_of_function_gemini_imagen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "z4atdPg-V2IY"
   },
   "outputs": [],
   "source": [
    "# Make an image from an prompt (openai)\n",
    "def generate_image_with_openai(prompt):\n",
    "    # Generate image\n",
    "    image_response = openai.Image.create(\n",
    "        prompt=prompt,\n",
    "        n=1,\n",
    "        size=\"1024x1024\"\n",
    "    )\n",
    "\n",
    "    # Get the image URL\n",
    "    response = image_response['data'][0]['url']\n",
    "    return response\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4PkOBQ7fWLGg"
   },
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Billing hard limit has been reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate the image with OpenAI\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_image_with_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_of_function_gemini_imagen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Download the image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(image_url)\n",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m, in \u001b[0;36mgenerate_image_with_openai\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_image_with_openai\u001b[39m(prompt):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Generate image\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     image_response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1024x1024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Get the image URL\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m image_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\generacion-de-prompts\\lib\\site-packages\\openai\\api_resources\\image.py:39\u001b[0m, in \u001b[0;36mImage.create\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     29\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m     30\u001b[0m     api_key,\n\u001b[0;32m     31\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m api_type, api_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_api_type_and_version(api_type, api_version)\n\u001b[1;32m---> 39\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mazure_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubmit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_type \u001b[38;5;129;01min\u001b[39;00m (util\u001b[38;5;241m.\u001b[39mApiType\u001b[38;5;241m.\u001b[39mAZURE, util\u001b[38;5;241m.\u001b[39mApiType\u001b[38;5;241m.\u001b[39mAZURE_AD):\n\u001b[0;32m     44\u001b[0m     requestor\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# operation_location is a full url\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\generacion-de-prompts\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\.conda\\envs\\generacion-de-prompts\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\generacion-de-prompts\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: Billing hard limit has been reached"
     ]
    }
   ],
   "source": [
    "# Generate the image with OpenAI\n",
    "image_url = generate_image_with_openai(response_of_function_gemini_imagen)\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(image_url)\n",
    "\n",
    "# Verify if possible to download\n",
    "if response.status_code == 200:\n",
    "    # Get filename without extension\n",
    "    filename = os.path.basename(image_url).split('.')[0]\n",
    "    # Add the extension \".png\"\n",
    "    filename += \".png\"\n",
    "    \n",
    "    # Directory to save the image\n",
    "    directory = \"./generated_images/\"\n",
    "    \n",
    "    # Make directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Complete path\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # Save the image\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(\"La imagen ha sido descargada y almacenada correctamente en:\", filepath)\n",
    "    \n",
    "    # Display the image using PIL\n",
    "    img = Image.open(filepath)\n",
    "    print(f\"Prompt utilizado: {response_of_function_gemini}\")\n",
    "    display(img)\n",
    "\n",
    "else:\n",
    "    print(\"Hubo un error al descargar la imagen.\")\n",
    "\n",
    "# Output the image URL\n",
    "print(f\"URL OpenAI: {image_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
